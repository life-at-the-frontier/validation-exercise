---
html:
  embed_local_images: true
---

# An experiment using residents and experts to evaluate the validity of a border detection algorithm.

Meng Le Zhang$^1$$^*$, Aneta Piekut, Zaneb Rasool, Lydia Warden, and Gwilym Pryce

[1]
[2]

* Corresponding Author
Email: meng_le.zhang@sheffield.ac.uk (MZ) 


-----

# Using residents and experts validate a border detection algorithm
Full title:  An experiment using residents and experts to evaluate the validity of a border detection algorithm.

Authors: Meng Le Zhang, Aneta Piekut, Zaneeb Rasool, Lydia Warden, and Gwilym Pryce
Versions:
- Original- 29 July 2021

Study preregistration: https://osf.io/yzw8u/ (29/07/2021)

# Introduction

A number of studies have used computer algorithms with areal-level data to detect community borders. One such study (Dean et al. @2019) uses a statistical model to detect sharp community boundaries, referred to as social frontiers. However, to date, none of these studies has validated the accuracy of their algorithms externally using human participants or other means. Therefore, the evidence base for the usefulness of these algorithms for policy and researchers has not been developed yet. Furthermore, data and algorithmic issues could lead to i) misleading inferential results using these borders and ii) misinformation once the results of these algorithms are made available to the public and policymakers. The study pre-registered and protocol is available at [https://osf.io/yzw8u/ (29/07/2021)].

## Literature review

The existence of a boundary between two communities could lead to several positive or negative outcomes. Past studies in Northern Ireland, where a concept of “interface areas” was coined have demonstrated that frontiers are contested areas, “spaces where the terrain is marked in ways that reflect the specific claims of competing groups” (Leonard 2006: 227), of less frequent interaction resulting in poorer familiarity with each other (Boal and Livingstone (1984), and as such, more likely to be sites of tensions and conflict. Because social forces represent wider social inequalities, including ethnic hierarchies, they are never settled and fixed, hence might provide “an excellent opportunity for mutual interpenetration and sway” (Kristof 1959: 273). This dual nature of the frontiers comes in the results of quantitative geography studies. While Legewie and Schaeffer (@2016) study in New York, USA, revealed more neighbourhood tension when boundaries were blurred and and between-community boundaries less defines, but Dean et al. (@2019) found more crime closes to more steep frontiers in Sheffield, UK. However, when repeating the study in Czechia using Dean et al. (@2019) method Křížková et al. (@2021) did not found significant relationship between the location of frontiers and crime. Maguire, French, and O’Reilly (@2016) did not used any boundary-detection algorithm, but found correlation between ‘peace lines’ and poorer mental health (higher depression and axiety).

Studies using algorithms to detect community boundaries have found that these boundaries are associated with more or less crime/ public disorder. Legewie and Schaeffer (@2016) use an edge detection algorithm from image processing whilst Dean et al. (@2019) chose borders based on a Bayesian statistical model. However, neither study validated their algorithm nor described how they picked most of the tuning parameters in their model.

[Insert Aneta's review]

## Rationale for a trial
The need for external validation of the results of algorithms is well accepted in fields like machine learning. Similarly, the validation of measurement instruments is commonplace in psychology. The lack of external validation leads to misleading results as well as a range of scientific issues ranging from measurement error to fundamental scientific issues such as a lack of falsifiability. This trial will establish an evidence base for the external validity of these detection algorithms starting with the sharp border detection algorithm in Dean et al. (@2019). The measure of sharpness is based on the proportion of foreign-born residents on either side of a border in 2011. Sharper borders are theorised to indicate the location of strong community borders or social frontiers.

The main claim behind boundary detection studies is that these boundaries can be detected using secondary data analysis (Legewie and Schaeffer 2016, Dean et al 2019). The same studies will implicitly or explicitly claim that these community boundaries are at least noticeable to people in the local area. [insert list quotes].

If both claims are true, it should be possible for local residents to distinguish community boundaries, generated by boundary detection algorithms, from other types of boundaries (e.g. random noise). This is a testable hypothesis. Assuming the testing procedure is valid, if local residents cannot validate these boundaries (e.g. cannot distinguish generated boundaries from random noise) then one or both claims are false: either the generated boundaries are not accurate depictions or that community boundaries are unnoticeable to the local community (but somehow apparent to academics and computer untested algorithms).

# Objectives

_Primary objective_: Determine whether residents and experts perceive sharper borders (as determined by the algorithm) as indicative of more clearly defined community borders.

_Hypothesis_: The algorithm predicts which borders are more likely to be clearly defined community borders. The hypothesis is that participants will agree with the predictions of the algorithm.

_Secondary objective_: Determine the feasibility of this type of trial for future research.


# Research design

We use the boundaries detection algorithm to create three maps (A, B, and C) of the same geographical area. The maps are ranked in order of how much they reflect sharp community boundaries (according to the algorithm, A > B > C). The maps then form the basis of our validation exercise with human participants.

The study was designed to be a multi-site trial across six communities in the UK, Norway and Sweden. In each site, particiants would be recruited from experts and residents from the local community. Participants are given a discrete choice given a pair of maps (e.g. map A and B, C and B etc) and asked to pick which map represents more defined community borders. Each participant is given three pair sets to complete. The pair sequence shown to particiants (and the order of maps within each pair) are randomised. The exercise is conducted as part of a longer semi-structured qualitative interview. The exercise is given at either the beginning or end of the interview (depending on whether experts or residents).

For each map pair, there is agreement if participants choose the map with sharper boundaries (as defined by the algorithm, e.g. A over B). Our test statistic is the rate of agreement: under the null hypothesis the agreement rate is 50 percent.

Due to practical issues, we were only able to collect data from one site. This leads to less statistical power than anticipated in the original protocol (e.g. lower likelihood of rejecting the null hypothesis). A study protocol including analysis plan was created before data collection [https://osf.io/yzw8u/].

# Methods


## Study setting


The study is carried out as part of a qualitative study with residents and experts living within areas with a high number of sharp community boundaries (as detected by the algorithm). In the study protocol, the proposed areas were Rotherham (England), Derry (Northern Ireland), 2 undetermined sites in Norway and 2 undetermined sites in Sweden.

A number of factors determined the choice of areas. First, each city was chosen due to their higher than average degree of segregation, the proportion of foreign-born residents, and the research team's prior links and collaborations. Rotherham – a town in South Yorkshire, and (London)Derry in Northern Ireland were selected as two case studies on the basis of literature and conversations with local stakeholders. As of 2011 Census, 92% Rotherham population was White British; it has a substantial settled British Pakistani population and more recent, i.e. post-2004 EU extension immigration from Eastern Europe. While (London)Derry’s segregation is largely along the religion line (Catholics vs. Protestants), we wanted to see to what extent any new emerging frontiers along the migration status and ethnicity lines were visible to residents. As such, the selection of two sides with social frontiers was purposeful in order to make the fieldwork more time efficient. We decided on the final boundaries of the studied areas after producing maps with social frontiers for Rotherham West and (London)Derry (based on 2011 Census data – add footnote?). MZ gave AP a map of social frontiers in each city. Finally, AP and expert ZR chose an area in Rotherham with an above-average number of defined community borders and MO and GH in Derry. The maps used in the trial are centred on the middle of these areas and show border information within a 1.5km radius.
Ethical approval from the University of Sheffield was secured on 30th July 2021.


The originally planned timeline of data collection was:
- Phase 1: Expert interviews (5 x 2 areas - July-August 2021) - online
- Phase 2: Individual interviews with residents (10-12 x 2 areas - August-October 2021) - face-to-face or online.
- Phase 3: Focus group interviews, FGIs (2 x 2 areas - November-December 2021) - face-to-face - focus group interviews with young people (aged 18-25) of each selected area; 4-5 participants in each group.
The fieldwork was disrupted and delayed by travel restrictions during the Covid-19 pandemic, and we decided to focus our resources on the Rotherham case – much more accessible from Sheffield.
- Phase 1: Expert interviews (5 interviews – 9 September-2 November 2021) - online
- Phase 2: Individual interviews with residents (XX interviews – 25 October 2021 – XX September 2022) - face-to-face.
- Phase 3: Focus group interviews, FGIs (??? - September 2022) - face-to-face – TBC as we planned to focus on immigrants in group interviews.

Table 1. Summary of socio-demographic profile of respondents
[insert]


## Eligibility criteria

Participants fall under three different groups.
- Local experts (adults, 18+) recruited among local community workers, housing associations, advocacy groups, third sector organisations, and various agencies involved in the support and integration of migrant communities in each selected case study area.
- Residents (adults, 18+) for individual interviews will be residents of the case study area who live in the vicinity and on different sides of the identified social frontier. We will invite residents living 1 km radius from the frontier, who have lived in the area for at least a year.
- [REVISE] Focus groups interviews will be conducted with young people aged 18-25 who have lived in the vicinity and on different sides of the identified social frontier for at least a year.


## Recruitment
- Local experts for expert interviews will be recruited via our local contacts in Rotherham and Derry, identified by desk research and by snowball sampling (i.e. further contacts gained from initial interviews).

- Residents of two selected areas. We will ask each expert to point us to any community webpages, groups or centres which are most suitable to recruit participants. We will create a leaflet with information about the research to be distributed via identified channels, online and offline, and interested participants can come back to us via email and telephone.

- Group interviews participants - will be recruited via our local contacts in Rotherham (RCUST) and Derry (i.e. The North West Migrants Forum).



## Validation task

After a preliminary task to familiarise participants with the map interface, where participants are shown pairs of interactive maps with borders (Map A and Map B). Participants are asked to choose which map better represents distinct community boundaries in terms of migration background in the selected area. The full description of the intervention and allocation mechanism is given in a separate document [here](#study-protocol-assignment.md).

Pilot exercises were conducted before deployment, and the entire validation exercise takes under 10 minutes to complete.

[@ fill in how the maps were created after the results are collected]

We will check that all the maps are similar with respect attributes (e.g. total border lengths) to omit alternative explanations (see supplementary materials). The pair of interactive maps are synced so scrolling and moving Map A also moved the area viewed in Map B.

The maps are created in R and exported as leaflet maps within html files. The code to recreate the maps are found at: https://github.com/MengLeZhang/wardenProject2021.





## Assignment of maps

**Allocation**
All participants will be shown all three pairs of maps after an initial preliminary task. The order of maps and the sequence of pairs is chosen at random by the statistical software R.

**Concealment mechanism and blinding**
The order and sequence is saved onto a file and not shown to the any member of the research team until the data has been collected. This includes member of the data analysis team who will only access the sequencing information after an interview to check the automated routine has not failed. The interview team, they will not see the order/sequence until all the data has been collected. The participants and the interview team will be unaware of how the maps in the exercise have been generated.

## Outcomes
The main outcome are the preferred maps chosen by each participant. This is recorded by the interview team along (with the entire semi-structured interview). Metadata about the exercise (e.g. how long participants spent on each task) was also recorded. Data about the random ordering of the exercise is automatically generated and saved using R.

## Sample size
The validation exercise was created as an add-on to a longer qualitative interview. Therefore, the sample size determined by the qualitative research design. In the protocol, the original sample was composed of respondents from six areas chosen across three countries (see below). Each selected area will have 15 participants involved in one-to-one interviews (five experts and ten residents). A simulation of 15 participants (e.g. single site only) shows that we would be able to detect an agreement rate of 73% (single map pair) at the usual level of statistical significance ($p < 0.05$).

[The final study?]


## Data collection and statistical analysis

_Collection and management_: Trial results data were the interview team (AP, ZR) and entered into a spreadsheet. The interview team checked a subset of results against the interview recordings for accuracy. We plan to collect basic demographic information on participants in the residents interview [@Aneta did we?].

_Statistical analysis_: There are three sets of map pairs comparing:

[XXX]

For each pair, we measure the agreement rate: proportion of participants who prefer the map with the sharpest borders. We wil refer this rate as $P_j$ where $j$ denotes map pairing. We calculate $Z_j$ where $Z_j =  P_{j} - 0.5$. Under the null hypothesis, $Z_j$ will be approximately normally distributed with a mean of 0 and standard error of $\sqrt{0.25 / n}$ (given our sample size is large enough, n > 25). We use this distribution to calculate our $p$ values with $p < 0.05$ as our threshold for statistical significance. With our final sample, for each map pair, agreement rates higher than 69.6% (or lower than 30.4%) are statistically significant ($p < 0.05$). We can also calulate the overall agreement rate across all map pairs (i.e. mean of $Z_j$).

Missing data, including item non-response (e.g. skipping pairs), will be omitted. We will test the effect of item non-response by using partial identification to calculate the lower and upper bounds for the agreement rate (e.g. non-responders are always agree or disagree).

We conduct additional tests for sequencing, ordering, and better agreement over time. Sequencing tests for whether participant engagement wanes during the exercises due to distractions or boredom (which will reduce the agreement rate). If engagement does not wane then the sequence that which pairs are shown should not affect agreement (e.g. pair 1 should not have a higher amount of agreement than pair 3). Ordering tests whether participants are more or less likely to pick the left map in a pair: this may suggest that participants want to complete the exercise quickly (e.g. lack of engagement). Under the null hypothesis, the chance of participants picking the left map is equal to the chance that the left map contains the 'correct' response. Finally we test whether agreement rates change over time. Higher agreement rate for later participants suggests that the interview team may be nudging participants towards 'correct' response (despite our attempt to blind the interview team). Since the order of participation is not random, this test could also reflect changes over time in participation (e.g. expert interviews occurred first, later participant may be less willing).

## Ethics and consent

We will sent participant information sheets well before the interviews (either via email or traditional post), so participants will have time to read them and ask questions. Consent for online interviews with experts will be given in through ticking a box on a Google Form. From experience in our past research conducted during the Covid-19 pandemic, we know that some participants find it difficult to add a signature to an online document. It will be a strict prerequisite for interviews to take place, and interviews will only proceed once consent has been established. Participants will be sent a digital copy of a consent form. For individual and group interviews, we will use either an online, Google Form consent or a printed / paper consent form (depending on what kind of interview will occur). Information about research will be distributed through identified via experts local community pages, emails (if participants approach us to take part in research via their email), and consent will be given either online or by signing a consent form. Participants will be sent a digital copy or given a paper copy, depending on the type of consent form they will sign.

Each participant of the individual and group interview will be offered a £25 shopping voucher in recognition of their time. The vouchers can be sent in a digital form if any interview happens online, sent to a home address or handed in person. Receipts will be collected in a digital or paper form, respectively. There will be no incentives for expert interviews.

In the context of the pandemic, many interview participants constitute a potentially vulnerable group given the increased risk of exposure to coronavirus, which might intersect with precarious professional and personal circumstances. With this in mind, the fieldwork will be informed by project partners, including community and migrant organisations with experience of supporting migrants in vulnerable situations (RCUST in Rotherham and the North West Migrants Forum in Derry, via Maeve O'Brien).

Participants could find parts of the interview distressing, e.g. emotionally sensitive issues such as migration status, place attachment and/or stigma, local conflicts, anxieties related to Brexit, or experiences of discrimination. This will be mitigated by sensitive interviewing, creating a supportive and relaxed interview experience, appropriate signposting and the right to withdraw from the study. Online interviews, where body language and non-verbal cues are harder to read, will also be informed by insights from project partners with experience of supporting migrants.

In addition, the presence of others (co-workers, family members) in the room during the interview might impact the conversation. To meet these challenges, the interviewer will inform the participants that the interview can be paused at any time and ask participants to, if possible, find a private space.

----

[here's some fake results jus to have something to write about]


# Results

The agreement rates for each map pair are: 40% (p = 0.317, pair 1), 32% (pair p = 0.2) and 48% (pair 3, p = 0.841). The overall agreement is 40%. In all cases, we cannot reject the null hypothesis that local residents do not recognise frontiers as more distinct community borders (compared to borders with lower levels of sharpness).

Testing the robustness of our study, we do not find any statistically significant sequencing effects (fisher exact test, p = 0.145); ordering effectts (fisher exact test, p = 0.157); or changes in agreement rate over time (p = 0.640). Our robustness tests combines responses from all map pairs and do not adjust for clustering (i.e. each participant contributes 3 responses). The results do not change, we do tests individually for each map pair.

All our results tables are included in the supplementary materials.


# Discussion

To our knowledge, experiments in Urban Studies and Human Geography are rare and this is one of a few studies to empirically test the validity of border detection algorithms. The study does have a number of limitations. The sample size is much smaller than initially anticipated which limits our ability to detect smaller effect sizes. Although we do not find any issues with the robustness of the study design, this does not mean issues are not present. Finally, frontier may be valid but local residents are unable to distinguish them. As mentioned before, this seems contrary to the theoretical statement put forward by Legewie and Schaeffer and Dean et al.

Further work can be done to scale up the validation exercise in order to detect smaller differences in agreement rates. Furthermore, the same research design can be used to establish the validity of different border detection algorithm against each other.

# Conclusions

Recap


## Declaration of interests
AP and GP are co-authors on the Dean et al. (2019) paper. We have a collaboration contract with Rotherham United Community Sports Trust (RUCUST) as a vendor. ZR works as a community manager at RUCUST.


## Data access
We intend to anonymise and share the final results data after an embargo period. The code to replicate the research materials, writeup, and statistical analysis can be found at: https://github.com/MengLeZhang/wardenProject2021. Through GitHub, other researchers can check changes made to the code and materials.

## Author's contributions

MZ originally conceived the validation study. AI (Aarti Iyer) and AP designed the qualitative interviews, sampling design and the recruitment of participants. AP, ZR and MO will conduct the interviews. LW and MZ created the research materials for the exercise. MZ, AP and LW are responsible for the write up of the study protocol and results. MZ provides statistical expertise and primary data analysis. AP and GP are grant holders.

## Funding

This study is funded by Nordforsk as part of the life at the frontiers project (project number 95193). For more details, see [here](https://www.nordforsk.org/projects/life-frontier-impact-social-frontiers-social-mobility-and-integration-migrants). LW's time on the study is funded by Sheffield University.

# References

Dean, Nema, Guanpeng Dong, Aneta Piekut, and Gwilym Pryce. 2019. ‘Frontiers in Residential Segregation: Understanding Neighbourhood Boundaries and Their Impacts’. Tijdschrift Voor Economische En Sociale Geografie 110 (3): 271–88. https://doi.org/10.1111/tesg.12316.

Legewie, Joscha, and Merlin Schaeffer. 2016. ‘Contested Boundaries: Explaining Where Ethnoracial Diversity Provokes Neighborhood Conflict’. American Journal of Sociology, 37.


----

# Supplementary materials

# Research materials
## Preliminary exercise

[screenshot]

## Validation exercise

[screenshot]

## Map attributes

[table here]


# Result tables

````

# Agreement

realPair     n agreeN agreeRate    se p.value
   <int> <int>  <int>     <dbl> <dbl>   <dbl>
1        1    25     10      0.4    0.1  0.317
2        2    25      8      0.32   0.1  0.0719
3        3    25     12      0.48   0.1  0.841

# Sequence

seenOrder agreeN disagreeN agreeRate
    <int>  <int>     <int>     <dbl>
1         1     14        11      0.56
2         2      9        16      0.36
3         3      7        18      0.28

Fisher's Exact Test for Count Data

data:  .
p-value = 0.1453
alternative hypothesis: two.sided

# order
mapA_position result1 result2
        <int>   <int>   <int>
1             1      15      16
2             2      29      15

Fisher's Exact Test for Count Data

data:  .
p-value = 0.157
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
0.1705639 1.3755335
sample estimates:
odds ratio
0.489756

# over tinme

splitTime agreeN disagreeN
<lgl>      <int>     <int>
1 FALSE         13        23
2 TRUE          17        22

Fisher's Exact Test for Count Data

data:  .
p-value = 0.6379
alternative hypothesis: true odds ratio is not equal to 1
95 percent confidence interval:
0.259976 2.041504
sample estimates:
odds ratio
0.7345308


````
